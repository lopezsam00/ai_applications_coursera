{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d9ead3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# IBM WatsonX imports\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4501d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(prompt_txt, params=None):\n",
    "    \n",
    "    model_id = \"ibm/granite-3-2-8b-instruct\"\n",
    "    \n",
    "    default_params = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"min_new_tokens\": 0,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "    \n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "    \n",
    "    # Set up credentials for WatsonxLLM\n",
    "    url = os.getenv(\"IBM_URL_END_POINT\")\n",
    "    apikey = os.getenv(\"IBM_API_KEY\")\n",
    "    username = os.getenv(\"WATSONX_USERNAME\")\n",
    "    project_id = os.getenv(\"IBM_PROJECT_ID\")\n",
    "    \n",
    "    # Create LLM directly\n",
    "    granite_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        apikey=apikey,\n",
    "        username=username,\n",
    "        project_id=project_id,\n",
    "        params=default_params,\n",
    "        url=url\n",
    "    )\n",
    "    \n",
    "    response = granite_llm.invoke(prompt_txt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3291cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f6ee283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: I have a really cute cockatiel that loves to \n",
      "\n",
      "response : ride on my shoulder. I want to take him with me on a trip to the beach. Is it safe for him to be there?\n",
      "\n",
      "While it's tempting to bring your cockatiel along for a beach trip, it's important to consider several factors to ensure his safety and well-being. Here are some points to keep in mind:\n",
      "\n",
      "1. **Temperature and Humidity:** Cockatiels are sensitive to extreme temperatures and high humidity. The beach environment can be quite hot and humid, which may cause heat stress or respiratory issues for your bird.\n",
      "\n",
      "2. **Noise and Crowds:** Beaches are often noisy and crowded, which can be overwhelming and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 158,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "\n",
    "prompt = \"I have a really cute cockatiel that loves to \"\n",
    "\n",
    "# Getting a reponse from the model with the provided prompt and new parameters\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cfcd5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Classify the following statement as true or false: \n",
      "            'The Eiffel Tower is located in Berlin.'\n",
      "\n",
      "            Answer:\n",
      "\n",
      "\n",
      "response : \n",
      "False. The Eiffel Tower is located in Paris, France.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Zero shot prompt\n",
    "prompt = \"\"\"Classify the following statement as true or false: \n",
    "            'The Eiffel Tower is located in Berlin.'\n",
    "\n",
    "            Answer:\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1c822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MOVIE_REVIEW RESPONSE ===\n",
      ". The acting was superb, and the soundtrack was hauntingly beautiful. However, the pacing was a bit slow, and the ending was somewhat predictable.'\"\n",
      "\n",
      "The movie review is positive. The reviewer praises the visuals, storyline, acting, and soundtrack, using words like \"stunning,\" \"gripping,\" \"superb,\" and \"hauntingly beautiful.\" Despite mentioning some drawbacks, such as slow pacing and a predictable ending, the overall tone and specific positive comments indicate a favorable opinion of the movie.\n",
      "\n",
      "=== CLIMATE_CHANGE RESPONSE ===\n",
      " weather events, and threats to biodiversity.\n",
      "\n",
      "Climate change, driven by human activities like fossil fuel burning and deforestation, results in significant global temperature shifts, severe weather events, and biodiversity threats, despite its natural occurrence.\n",
      "\n",
      "=== TRANSLATION RESPONSE ===\n",
      "\n",
      "\n",
      "Translation: 'Technology advances rapidly in the modern world.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Starter code: provide your solutions in the TODO parts\n",
    "\n",
    "# 1. Prompt for Movie Review Classification\n",
    "movie_review_prompt = \"\"\"\"Determine if the following movie review is positive or negative:\n",
    "'The movie had stunning visuals and a gripping storyline that kept me on the edge of my seat\"\"\"\n",
    "\n",
    "# 2. Prompt for Climate Change Paragraph Summarization\n",
    "climate_change_prompt = \"\"\"Summarize the following paragraph about climate change in one sentence:\n",
    "Climate change refers to significant changes in global temperatures and weather patterns over time. \n",
    "While climate change is a natural phenomenon, \n",
    "scientific evidence shows that human activities, \n",
    "particularly the burning of fossil fuels and deforestation, have accelerated the process. \n",
    "The consequences of climate change include rising sea levels, more frequent and severe\"\"\"\n",
    "\n",
    "# 3. Prompt for English to Spanish Translation\n",
    "translation_prompt = \"\"\"Traduzca la siguiente oracion al ingles: 'La tecnologia avanza rapidamente en el mundo moderno.'\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"movie_review\"] = llm_model(movie_review_prompt, params)\n",
    "responses[\"climate_change\"] = llm_model(climate_change_prompt, params)\n",
    "responses[\"translation\"] = llm_model(translation_prompt, params)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a83193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Here is an example of translating a sentence from English to French:\n",
      "\n",
      "            English: “How is the weather today?”\n",
      "            French: “Comment est le temps aujourd'hui?”\n",
      "\n",
      "            Now, translate the following sentence from English to French:\n",
      "\n",
      "            English: “Where is the nearest supermarket?”\n",
      "\n",
      "\n",
      "\n",
      "response :  French: “Où est le supermarché le plus proche?”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here is an example of translating a sentence from English to French:\n",
    "\n",
    "            English: “How is the weather today?”\n",
    "            French: “Comment est le temps aujourd'hui?”\n",
    "            \n",
    "            Now, translate the following sentence from English to French:\n",
    "            \n",
    "            English: “Where is the nearest supermarket?”\n",
    "            \n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09224715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORMAL_EMAIL RESPONSE ===\n",
      "\n",
      "Subject: Gui - The Feathered Marvel\n",
      "\n",
      "Body:\n",
      "\n",
      "Dear [Colleague's Name],\n",
      "\n",
      "I hope this email finds you well. I wanted to share some delightful observations about our other avian colleague, Gui.\n",
      "\n",
      "Just like Giannis, Gui is a true marvel in the bird world. His vibrant plumage and lively demeanor never fail to brighten up our workspace. He's not just a bird; he's a feathered marvel, a testament to the beauty\n",
      "\n",
      "=== TECHNICAL_CONCEPT RESPONSE ===\n",
      "\n",
      "1. I have a green parrot. She enjoys mimicking sounds. She is very intelligent.\n",
      "2. My green parrot enjoys mimicking sounds; she is very intelligent.\n",
      "3. The green parrot I have enjoys mimicking sounds, and she is very intelligent.\n",
      "4. Green parrot: she enjoys mimicking sounds, and she is very intelligent.\n",
      "5. She, my green parrot, enjoys mimicking sounds, and she is very\n",
      "\n",
      "=== KEYWORD_EXTRACTION RESPONSE ===\n",
      "\n",
      "1. Giannis\n",
      "2. Giannis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Starter code: provide your solutions in the TODO parts\n",
    "params = {\n",
    "    \"max_new_tokens\": 120,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "# 1. One-shot prompt for formal email writing\n",
    "formal_email_prompt = \"\"\"prompt: Write an email to a colleague like this\n",
    "Subject - Giannis is a bird; Body - He is a super cuitie lil nugg\n",
    "Write one about my other bird gui\n",
    "\"\"\"\n",
    "\n",
    "# 2. One-shot prompt for simplifying technical concepts\n",
    "technical_concept_prompt = \"\"\"\"\n",
    "One way that AI can be caught is the use of '-' between thoughts\n",
    "\n",
    "here is an example,\n",
    "ai response: I have a white cockatiel - he loves to sing and dance - he is very friendly\n",
    "correction: I have a white cockatiel. He loves to sing and dance. He is very friendly.\n",
    "\n",
    "fix this ai response: I have a green parrot - she enjoys mimicking sounds - she is very intelligent\n",
    "\"\"\"\n",
    "\n",
    "# 3. One-shot prompt for keyword extraction\n",
    "keyword_extraction_prompt = \"\"\"\"\n",
    "Extract the word Giannis from the following sentence:\n",
    "\"The quick Giannis fox jumps over the lazy dog.\"\n",
    "\n",
    "Do the same for this sentence:\n",
    "\"The energetic Giannis bird plays in the park.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"formal_email\"] = llm_model(formal_email_prompt, params)\n",
    "responses[\"technical_concept\"] = llm_model(technical_concept_prompt, params)\n",
    "responses[\"keyword_extraction\"] = llm_model(keyword_extraction_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d152cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Here are few examples of classifying emotions in statements:\n",
      "\n",
      "            Statement: 'I just won my first marathon!'\n",
      "            Emotion: Joy\n",
      "\n",
      "            Statement: 'I can't believe I lost my keys again.'\n",
      "            Emotion: Frustration\n",
      "\n",
      "            Statement: 'My best friend is moving to another country.'\n",
      "            Emotion: Sadness\n",
      "\n",
      "            Now, classify the emotion in the following statement:\n",
      "            Statement: 'That movie was so scary I had to cover my eyes.’\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "response :  Emotion: Fear\n",
      "\n",
      "Explanation: The\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#parameters: Set `max_new_tokens` to 10, which constrains the model to generate brief responses\n",
    "\n",
    "params = {\n",
    "    \"max_new_tokens\": 10,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here are few examples of classifying emotions in statements:\n",
    "\n",
    "            Statement: 'I just won my first marathon!'\n",
    "            Emotion: Joy\n",
    "            \n",
    "            Statement: 'I can't believe I lost my keys again.'\n",
    "            Emotion: Frustration\n",
    "            \n",
    "            Statement: 'My best friend is moving to another country.'\n",
    "            Emotion: Sadness\n",
    "            \n",
    "            Now, classify the emotion in the following statement:\n",
    "            Statement: 'That movie was so scary I had to cover my eyes.’\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b3cd021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
      "            How many apples are there now?’\n",
      "\n",
      "            Break down each step of your calculation\n",
      "\n",
      "\n",
      "\n",
      "response : \n",
      "1. Start with the initial number of apples: 22 apples.\n",
      "2. Subtract the number of apples sold today: 22 - 15 = 7 apples remaining.\n",
      "3. Add the new delivery of apples: 7 + 8 = 15 apples now.\n",
      "\n",
      "So, there are 15 apples in the store now.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
    "            How many apples are there now?’\n",
    "\n",
    "            Break down each step of your calculation\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c026311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DECISION_MAKING RESPONSE ===\n",
      "\n",
      "To decide between a beach or mountain weekend trip, I'll consider the following factors:\n",
      "\n",
      "1. Weather: I'll check the forecast for both locations to ensure favorable conditions for outdoor activities.\n",
      "2. Activities available: I'll research the types of activities each location offers, such as hiking, swimming, or sightseeing.\n",
      "3. Travel time: I'll compare the time it takes to reach each destination from my current location.\n",
      "\n",
      "=== SANDWICH_MAKING RESPONSE ===\n",
      "\n",
      "1. Gather your ingredients: You'll need two slices of bread, a few slices of turkey, some lettuce leaves, a tomato, and mayonnaise.\n",
      "\n",
      "2. Prepare the bread: Place the two slices of bread on a clean surface. If you prefer, you can lightly toast the bread in a toaster for added crunch.\n",
      "\n",
      "3. Spread the mayonnaise: Take a spoon or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Starter code: provide your solutions in the TODO parts\n",
    "params = {\n",
    "    \"max_new_tokens\": 100,\n",
    "}\n",
    "# 1. Prompt for decision-making process\n",
    "decision_making_prompt = \"\"\"\n",
    "You are planning a weekend trip and need to decide between going to the beach or the mountains.\n",
    "Consider factors such as weather, activities available, travel time, and personal preferences.\n",
    "Based on these considerations, explain your decision-making process and choose the best option.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prompt for explaining a process\n",
    "sandwich_making_prompt = \"\"\"\n",
    "Make a sandwich for me with turkey, lettuce, tomato, and mayo.\n",
    "Explain each step of the process.\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"decision_making\"] = llm_model(decision_making_prompt, params)\n",
    "responses[\"sandwich_making\"] = llm_model(sandwich_making_prompt, params)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e4dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
      "\n",
      "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
      "\n",
      "\n",
      "\n",
      "response :  Calculation 1:\n",
      "When I was 6, my sister was half my age, so she was 6 / 2 = 3 years old.\n",
      "Now, 70 - 3 = 67 years old.\n",
      "\n",
      "Calculation 2:\n",
      "If my sister was 3 when I was 6, then she is 6 - 3 = 3 years younger than me.\n",
      "Now, 70 - 3 = 67 years old.\n",
      "\n",
      "Calculation 3:\n",
      "My sister was 3 when I was 6, so she is 3 years younger than me.\n",
      "Now, 70 - 3 = 67 years old.\n",
      "\n",
      "All three calculations show that my sister is 67 years old now. This is the most consistent result, as it is derived from the same initial information and applies the same logic in each calculation.\n",
      "\n",
      "Final answer: My sister is 67 years old.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
    "\n",
    "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a342e3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WatsonxLLM(model_id='meta-llama/llama-3-405b-instruct', project_id='05d2d283-6b30-438a-8aa9-251ce686d37d', url=SecretStr('**********'), apikey=SecretStr('**********'), username=SecretStr('**********'), params={'max_new_tokens': 256, 'temperature': 0.5}, watsonx_model=<ibm_watsonx_ai.foundation_models.inference.model_inference.ModelInference object at 0x775e02635bb0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "model_id = \"meta-llama/llama-3-405b-instruct\"\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "url = os.getenv(\"IBM_URL_END_POINT\")\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        url = os.getenv(\"IBM_URL_END_POINT\"),\n",
    "        apikey = os.getenv(\"IBM_API_KEY\"),\n",
    "        username = os.getenv(\"WATSONX_USERNAME\"),\n",
    "        project_id = os.getenv(\"IBM_PROJECT_ID\")\n",
    "    )\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9bb7164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], template='Tell me a {adjective} joke about {content}.\\n')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Tell me a {adjective} joke about {content}.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cdc08ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d179333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a function to ensure proper formatting\n",
    "def format_prompt(variables):\n",
    "    return prompt.format(**variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd2f2b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the chicken go to the doctor?\n",
      "\n",
      "Because it had... (wait for it)... FOWL breath!\n",
      "\n",
      "(Sorry, I know it's a bit of a poultry joke, but I hope it cracked you up!)\n"
     ]
    }
   ],
   "source": [
    "# Create the chain with explicit formatting\n",
    "joke_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = joke_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94411352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the fish go to the party alone? Because he heard it was a \"reel\" good time, but he was hooked on his ex and couldn't sea a future without her. Now he's just a sole searching for love in a ocean of loneliness.\n"
     ]
    }
   ],
   "source": [
    "response = joke_chain.invoke({\"adjective\": \"sad\", \"content\": \"fish\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29fd656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the text in one sentence:\n",
      "\n",
      "The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation, through innovations like AI, machine learning, and the Internet of Things, leading to improved productivity and a more interconnected society.\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"\n",
    "    The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. \n",
    "    Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. \n",
    "    For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. \n",
    "    Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. \n",
    "    These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Summarize the {content} in one sentence.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "summarize_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "summary = summarize_chain.invoke({\"content\": content})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a34c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid.\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"\n",
    "    The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
    "    The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
    "    The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which planets in the solar system are rocky and solid?\"\n",
    "\n",
    "template = \"\"\"\n",
    "    Answer the {question} based on the {content}.\n",
    "    Respond \"Unsure about answer\" if not sure about the answer.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "qa_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "answer = qa_chain.invoke({\"question\": question, \"content\": content})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32422926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Music\n",
      "\n",
      "    Reasoning Skill: This question requires the ability to analyze the content of the sentence and identify the primary topic or theme. In this case, the sentence mentions a concert, artists, and performances, which are all related to music. Therefore, the correct classification is Music. This type of question requires the ability to recognize and categorize information, which is a key skill in Scientific Evidence Evaluation. \n",
      "\n",
      "    Note: The other options (Entertainment, Food and Dining, Technology, Literature) are not directly related to the content of the sentence, making Music the most appropriate classification. \n",
      "\n",
      "    Let me know if you want me to generate another question! \n",
      "\n",
      "    Also, I can generate a question that requires a higher level of reasoning skill, similar to the example you provided, where the correct answer is not immediately obvious and requires careful analysis of the information. Let me know! \n",
      "\n",
      "    Here is an example of a question that requires a higher level of reasoning skill:\n",
      "\n",
      "    Classify the \n",
      "    The new smartphone app allows users to track their daily caloric intake and provides personalized recommendations for healthy eating.\n",
      " into one of the Entertainment, Food and Dining, Technology, Literature, Music..\n",
      "\n",
      "    Category:\n",
      "\n",
      "    Food and Dining\n",
      "\n",
      "    Reasoning Skill: This question requires the\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "    The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
    "\"\"\"\n",
    "\n",
    "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
    "\n",
    "template = \"\"\"\n",
    "    Classify the {text} into one of the {categories}.\n",
    "    \n",
    "    Category:\n",
    "    \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "classification_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "category = classification_chain.invoke({\"text\": text, \"categories\": categories})\n",
    "print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "132eb980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SELECT c.name, c.email \n",
      "    FROM customers c \n",
      "    INNER JOIN purchases p \n",
      "    ON c.customer_id = p.customer_id \n",
      "    WHERE p.purchase_date >= NOW() - INTERVAL 30 DAY\n",
      "\n",
      "    Explanation:\n",
      "\n",
      "    This SQL query retrieves the names and email addresses of all customers who have made a purchase in the last 30 days. \n",
      "\n",
      "    Here's how it works:\n",
      "\n",
      "    1. The SELECT statement specifies the columns we want to retrieve: 'name' and 'email' from the 'customers' table.\n",
      "\n",
      "    2. The FROM clause specifies the tables we want to retrieve data from: 'customers' and 'purchases'. We assign aliases to these tables ('c' for 'customers' and 'p' for 'purchases') to make the query easier to read.\n",
      "\n",
      "    3. The INNER JOIN clause combines rows from the 'customers' and 'purchases' tables where the join condition is met. In this case, the join condition is that the 'customer_id' in the 'customers' table matches the 'customer_id' in the 'purchases' table.\n",
      "\n",
      "    4. The WHERE clause filters the results to only include rows where the 'purchase_date' in the 'purchases' table is within the last\n"
     ]
    }
   ],
   "source": [
    "description = \"\"\"\n",
    "    Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days. \n",
    "    The table 'purchases' contains a column 'purchase_date'\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "    Generate an SQL query based on the {description}\n",
    "    \n",
    "    SQL Query:\n",
    "    \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "sql_generation_chain = (\n",
    "    RunnableLambda(format_prompt) \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "sql_query = sql_generation_chain.invoke({\"description\": description})\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73cacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Review #1 ====\n"
     ]
    }
   ],
   "source": [
    "## Starter code: provide your solutions in the TODO parts\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# First initialize your LLM\n",
    "model_id = \"meta-llama/llama-3-3-70b-instruct\" ## Or you can use other LLMs available via watsonx.ai\n",
    "\n",
    "# Use these parameters\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 512,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.2, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "# TODO: Initialize your LLM\n",
    "llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        url = os.getenv(\"IBM_URL_END_POINT\"),\n",
    "        apikey = os.getenv(\"IBM_API_KEY\"),\n",
    "        username = os.getenv(\"WATSONX_USERNAME\"),\n",
    "        project_id = os.getenv(\"IBM_PROJECT_ID\")\n",
    ")\n",
    "\n",
    "# Here is an example template you can use\n",
    "template = \"\"\"\n",
    "Analyze the following product review:\n",
    "\"{review}\"\n",
    "\n",
    "Provide your analysis in the following format:\n",
    "- Sentiment: (positive, negative, or neutral)\n",
    "- Key Features Mentioned: (list the product features mentioned)\n",
    "- Summary: (one-sentence summary)\n",
    "\"\"\"\n",
    "\n",
    "product_review_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_review_prompt(variables):\n",
    "    return product_review_prompt.format(**variables)\n",
    "\n",
    "# TODO: Build your LCEL chain\n",
    "review_analysis_chain = (\n",
    "    RunnableLambda(format_review_prompt) \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example reviews to process\n",
    "reviews = [\n",
    "    \"I love this smartphone! The camera quality is exceptional and the battery lasts all day. The only downside is that it heats up a bit during gaming.\",\n",
    "    \"This laptop is terrible. It's slow, crashes frequently, and the keyboard stopped working after just two months. Customer service was unhelpful.\"\n",
    "]\n",
    "\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    print(f\"==== Review #{i+1} ====\")\n",
    "    result = review_analysis_chain.invoke({\"review\": review})\n",
    "    print(result)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
